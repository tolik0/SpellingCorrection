{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\')\n",
    "import nltk\n",
    "import os\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from scripts.attention_utils import softmax, one_hot\n",
    "from scripts.attention_preprocessing import transform_data\n",
    "from scripts.data_loader import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\data\\\\dicts\", 'rb') as file:\n",
    "    vocab_to_int, int_to_vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 200 # input sequence length\n",
    "Ty = 200 # output sequence length\n",
    "vocab_size = len(vocab_to_int) # number of unique characters\n",
    "n_a = 32 # number of neurons in single LSTM in encoder\n",
    "n_s = 64 # number of neurons in single LSTM in decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeat_layer = RepeatVector(Tx)\n",
    "concatenate_layer = Concatenate(axis=-1)\n",
    "dense_layer_1 = Dense(10, activation = \"tanh\")\n",
    "dense_layer_2 = Dense(1, activation = \"relu\")\n",
    "activation_layer = Activation(softmax, name='attention_weights')\n",
    "dot_prod_layer = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Args:\n",
    "    a (np.array): hidden state output of the Bi-LSTM, numpy-array of shape (?, Tx, 2*n_a)\n",
    "    s_prev (np.array): previous hidden state of the (post-attention) LSTM, numpy-array of shape (?, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context (np.array): context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeat_layer(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenate_layer([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e.\n",
    "    e = dense_layer_1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies.\n",
    "    energies = dense_layer_2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activation_layer(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dot_prod_layer([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model for training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_layer = Reshape((1, vocab_size))\n",
    "concatenate_layer_1 = Concatenate(axis=-1)\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(vocab_size, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder part of the model\n",
    "X = Input(shape=(Tx, vocab_size), name='X')\n",
    "a = Bidirectional(LSTM(n_a, return_sequences=True))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = Input(shape=(n_s,), name='s0')\n",
    "c0 = Input(shape=(n_s,), name='c0')\n",
    "Y_true = Input(shape=(Ty, vocab_size), name='Y_true')\n",
    "s = s0\n",
    "c = c0\n",
    "\n",
    "# Initialize empty list of outputs\n",
    "outputs = []\n",
    "\n",
    "for t in range(Ty):\n",
    "\n",
    "    # Perform one step of the attention mechanism to get back the context vector at step t\n",
    "    context = one_step_attention(a, s) # context.shape  = (?, 1, 2*n_a)\n",
    "    y_true = Lambda(lambda x: x[:, t, :])(Y_true) # y_true.shape = (?, vocab_size)\n",
    "    y_true = reshape_layer(y_true) # y_true.shape = (?, 1, vocab_size)\n",
    "    context = concatenate_layer_1([y_true, context])\n",
    "    # Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "    s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "\n",
    "    # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "    out = output_layer(s)\n",
    "\n",
    "    outputs.append(out)\n",
    "\n",
    "training_model = Model(inputs=[X, s0, c0, Y_true], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.02, beta_1=0.9, beta_2=0.999)\n",
    "training_model.compile(opt, 'categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "   59/18000 [..............................] - ETA: 64:28:42 - loss: 259.8138 - dense_3_loss: 0.0029 - dense_3_acc: 0.9775 - dense_3_acc_1: 0.5629 - dense_3_acc_2: 0.4502 - dense_3_acc_3: 0.3424 - dense_3_acc_4: 0.2464 - dense_3_acc_5: 0.2944 - dense_3_acc_6: 0.2815 - dense_3_acc_7: 0.2780 - dense_3_acc_8: 0.3256 - dense_3_acc_9: 0.3292 - dense_3_acc_10: 0.3485 - dense_3_acc_11: 0.3531 - dense_3_acc_12: 0.3614 - dense_3_acc_13: 0.3637 - dense_3_acc_14: 0.3500 - dense_3_acc_15: 0.3307 - dense_3_acc_16: 0.3402 - dense_3_acc_17: 0.3366 - dense_3_acc_18: 0.3461 - dense_3_acc_19: 0.3393 - dense_3_acc_20: 0.3398 - dense_3_acc_21: 0.3537 - dense_3_acc_22: 0.3520 - dense_3_acc_23: 0.3544 - dense_3_acc_24: 0.3624 - dense_3_acc_25: 0.3629 - dense_3_acc_26: 0.3625 - dense_3_acc_27: 0.3512 - dense_3_acc_28: 0.3527 - dense_3_acc_29: 0.3447 - dense_3_acc_30: 0.3617 - dense_3_acc_31: 0.3529 - dense_3_acc_32: 0.3607 - dense_3_acc_33: 0.3495 - dense_3_acc_34: 0.3664 - dense_3_acc_35: 0.3625 - dense_3_acc_36: 0.3551 - dense_3_acc_37: 0.3647 - dense_3_acc_38: 0.3644 - dense_3_acc_39: 0.3578 - dense_3_acc_40: 0.3675 - dense_3_acc_41: 0.3698 - dense_3_acc_42: 0.3815 - dense_3_acc_43: 0.3754 - dense_3_acc_44: 0.3658 - dense_3_acc_45: 0.3766 - dense_3_acc_46: 0.3771 - dense_3_acc_47: 0.3698 - dense_3_acc_48: 0.3675 - dense_3_acc_49: 0.3686 - dense_3_acc_50: 0.3690 - dense_3_acc_51: 0.3849 - dense_3_acc_52: 0.3815 - dense_3_acc_53: 0.3942 - dense_3_acc_54: 0.3927 - dense_3_acc_55: 0.3981 - dense_3_acc_56: 0.4025 - dense_3_acc_57: 0.4146 - dense_3_acc_58: 0.4022 - dense_3_acc_59: 0.4124 - dense_3_acc_60: 0.4327 - dense_3_acc_61: 0.4315 - dense_3_acc_62: 0.4356 - dense_3_acc_63: 0.4339 - dense_3_acc_64: 0.4473 - dense_3_acc_65: 0.4483 - dense_3_acc_66: 0.4575 - dense_3_acc_67: 0.4595 - dense_3_acc_68: 0.4683 - dense_3_acc_69: 0.4676 - dense_3_acc_70: 0.4620 - dense_3_acc_71: 0.4815 - dense_3_acc_72: 0.4766 - dense_3_acc_73: 0.4932 - dense_3_acc_74: 0.4910 - dense_3_acc_75: 0.4942 - dense_3_acc_76: 0.5003 - dense_3_acc_77: 0.5210 - dense_3_acc_78: 0.5144 - dense_3_acc_79: 0.5176 - dense_3_acc_80: 0.5175 - dense_3_acc_81: 0.5237 - dense_3_acc_82: 0.5259 - dense_3_acc_83: 0.5380 - dense_3_acc_84: 0.5376 - dense_3_acc_85: 0.5492 - dense_3_acc_86: 0.5414 - dense_3_acc_87: 0.5498 - dense_3_acc_88: 0.5544 - dense_3_acc_89: 0.5620 - dense_3_acc_90: 0.5705 - dense_3_acc_91: 0.5754 - dense_3_acc_92: 0.5786 - dense_3_acc_93: 0.5903 - dense_3_acc_94: 0.5775 - dense_3_acc_95: 0.5951 - dense_3_acc_96: 0.5980 - dense_3_acc_97: 0.5997 - dense_3_acc_98: 0.6031 - dense_3_acc_99: 0.6107 - dense_3_acc_100: 0.6114 - dense_3_acc_101: 0.6266 - dense_3_acc_102: 0.6249 - dense_3_acc_103: 0.6363 - dense_3_acc_104: 0.6373 - dense_3_acc_105: 0.6417 - dense_3_acc_106: 0.6500 - dense_3_acc_107: 0.6497 - dense_3_acc_108: 0.6510 - dense_3_acc_109: 0.6663 - dense_3_acc_110: 0.6666 - dense_3_acc_111: 0.6715 - dense_3_acc_112: 0.6741 - dense_3_acc_113: 0.6769 - dense_3_acc_114: 0.6793 - dense_3_acc_115: 0.6854 - dense_3_acc_116: 0.6903 - dense_3_acc_117: 0.6951 - dense_3_acc_118: 0.7022 - dense_3_acc_119: 0.6951 - dense_3_acc_120: 0.7007 - dense_3_acc_121: 0.7183 - dense_3_acc_122: 0.7151 - dense_3_acc_123: 0.7176 - dense_3_acc_124: 0.7271 - dense_3_acc_125: 0.7292 - dense_3_acc_126: 0.7229 - dense_3_acc_127: 0.7353 - dense_3_acc_128: 0.7307 - dense_3_acc_129: 0.7356 - dense_3_acc_130: 0.7415 - dense_3_acc_131: 0.7520 - dense_3_acc_132: 0.7476 - dense_3_acc_133: 0.7558 - dense_3_acc_134: 0.7610 - dense_3_acc_135: 0.7661 - dense_3_acc_136: 0.7661 - dense_3_acc_137: 0.7715 - dense_3_acc_138: 0.7736 - dense_3_acc_139: 0.7819 - dense_3_acc_140: 0.7832 - dense_3_acc_141: 0.7834 - dense_3_acc_142: 0.7817 - dense_3_acc_143: 0.7920 - dense_3_acc_144: 0.7932 - dense_3_acc_145: 0.7971 - dense_3_acc_146: 0.7995 - dense_3_acc_147: 0.8075 - dense_3_acc_148: 0.8107 - dense_3_acc_149: 0.8212 - dense_3_acc_150: 0.8117 - dense_3_acc_151: 0.8188 - dense_3_acc_152: 0.8232 - dense_3_acc_153: 0.8246 - dense_3_acc_154: 0.8264 - dense_3_acc_155: 0.8322 - dense_3_acc_156: 0.8315 - dense_3_acc_157: 0.8310 - dense_3_acc_158: 0.8347 - dense_3_acc_159: 0.8369 - dense_3_acc_160: 0.8429 - dense_3_acc_161: 0.8456 - dense_3_acc_162: 0.8512 - dense_3_acc_163: 0.8542 - dense_3_acc_164: 0.8568 - dense_3_acc_165: 0.8625 - dense_3_acc_166: 0.8532 - dense_3_acc_167: 0.8617 - dense_3_acc_168: 0.8607 - dense_3_acc_169: 0.8688 - dense_3_acc_170: 0.8669 - dense_3_acc_171: 0.8724 - dense_3_acc_172: 0.8719 - dense_3_acc_173: 0.8783 - dense_3_acc_174: 0.8780 - dense_3_acc_175: 0.8853 - dense_3_acc_176: 0.8831 - dense_3_acc_177: 0.8834 - dense_3_acc_178: 0.8808 - dense_3_acc_179: 0.8859 - dense_3_acc_180: 0.8898 - dense_3_acc_181: 0.8941 - dense_3_acc_182: 0.8936 - dense_3_acc_183: 0.8976 - dense_3_acc_184: 0.8980 - dense_3_acc_185: 0.9086 - dense_3_acc_186: 0.9039 - dense_3_acc_187: 0.9110 - dense_3_acc_188: 0.9173 - dense_3_acc_189: 0.9241 - dense_3_acc_190: 0.9308 - dense_3_acc_191: 0.9390 - dense_3_acc_192: 0.9514 - dense_3_acc_193: 0.9569 - dense_3_acc_194: 0.9686 - dense_3_acc_195: 0.9807 - dense_3_acc_196: 0.9956 - dense_3_acc_197: 0.9961 - dense_3_acc_198: 0.9981 - dense_3_acc_199: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-14e7a9034546>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m18000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtraining_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tutorial\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "filenames = [str(x) for x in range(18000)]\n",
    "training_model.fit_generator(generator=DataGenerator(filenames, filenames), steps_per_epoch=None, epochs=1, workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.save_weights(\"..\\\\models\\\\attention_model\\\\attention.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_inf = Input(shape=(1, vocab_size), name='Y_inf')\n",
    "s_inf = [s0 for i in range(B)]\n",
    "c_inf = [c0 for i in range(B)]\n",
    "y_inf = [Y_inf for i in range(B)]\n",
    "\n",
    "outputs_inf = [[] for i in range(B)]\n",
    "prob = [1 for i in range(B)]\n",
    "context_inf = [None for i in range(B)]\n",
    "out_inf = [None for i in range(B)]\n",
    "s_inf_new = [None for i in range(B)]\n",
    "c_inf_new = [None for i in range(B)]\n",
    "out_inf_new = [None for i in range(B)]\n",
    "outputs_inf_new = [None for i in range(B)]\n",
    "\n",
    "for t in range(Ty):\n",
    "        \n",
    "    for i in range(B):\n",
    "        context_inf[i] = one_step_attention(a, s_inf[i])\n",
    "        context_inf[i] = concatenate_layer_1([y_inf[i], context_inf[i]])\n",
    "        s_inf[i], _, c_inf[i] = post_activation_LSTM_cell(context_inf[i], initial_state=[s_inf[i], c_inf[i]])\n",
    "        \n",
    "        out_inf[i] = output_layer(s_inf[i])\n",
    "        \n",
    "        outputs_inf[i].append(out_inf[i])\n",
    "        out_inf[i]*=prob[i]\n",
    "        \n",
    "    if not t:\n",
    "        _, indices = tf.math.top_k(out_inf[0], k=B)\n",
    "    else:\n",
    "        concat = np.concatenate(out_inf, axis=-1)\n",
    "        _, indices = tf.math.top_k(concat, k=B)\n",
    "    \n",
    "    for i in range(B):\n",
    "        index = indices[i]//vocab_size\n",
    "        s_inf_new[i] = s_inf[index]\n",
    "        c_inf_new[i] = c_inf[index]\n",
    "        out_inf_new[i] = out_inf[index]\n",
    "        outputs_inf_new[i] = outputs_inf[index]\n",
    "        \n",
    "    for i in range(B):\n",
    "        s_inf[i] = s_inf_new[i]\n",
    "        c_inf[i] = c_inf_new[i]\n",
    "        out_inf[i]  = out_inf_new[i]\n",
    "        outputs_inf[i] = outputs_inf_new[i]\n",
    "        prob[i] = y_inf[i][indeces[i]%vocab_size]\n",
    "        y_inf[i] = tf.one_hot(indeces[i]%vocab_size, 56) \n",
    "        y_inf[i] = RepeatVector(1)(y_inf[i])\n",
    "\n",
    "concat = np.concatenate(out_inf, axis=0)\n",
    "index = np.argmax(concat)      \n",
    "inference_model = Model([X, s0, c0, Y_inf], outputs_inf[index//vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[X], outputs=[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_inference_model(vocab_size):\n",
    "    \"\"\"\n",
    "    Return decoder that makes one step inference\n",
    "    \n",
    "    Args:\n",
    "    vocab_size (int): number of distinct characters for output\n",
    "    Returns:\n",
    "    inference_model (Model): keras model that makes one step inference\n",
    "    \"\"\"\n",
    "    a = Input(shape=(Tx, 2*n_a), name='a')\n",
    "    Y_prev = Input(shape=(1, vocab_size), name='Y_inf')\n",
    "    y_prev = Y_prev\n",
    "    \n",
    "    context = one_step_attention(a, s0)\n",
    "    context = concatenate_layer_1([y_prev, context])\n",
    "    s, _, c = post_activation_LSTM_cell(context, initial_state=[s0, c0])\n",
    "    \n",
    "    out = output_layer(s)\n",
    "    \n",
    "    return Model(inputs=[a, s0, c0, Y_prev], outputs=[out, s, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(X, beam_width, vocab_size, Ty, n_s):\n",
    "    \"\"\"\n",
    "    Performs beam search among outputs of inference model\n",
    "    \n",
    "    Args:\n",
    "    X (numpy.ndarray): sentences in a form of numpy arrays\n",
    "    beam_width (int): number of best candidates to choose\n",
    "    vocab_size (int): number of distinct characters for output\n",
    "    Ty (int): number of time steps to perform\n",
    "    n_s (int): number of neurons in post attention LSTM\n",
    "    Returns:\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # batch_size\n",
    "    # get output of encoder\n",
    "    a = encoder.predict(x=[X])\n",
    "    # create beam_width instances of decoder\n",
    "    models = [one_step_inference_model(vocab_size) for _ in range(beam_width)]\n",
    "    prob = [1 for _ in range(beam_width)]\n",
    "    s = [np.zeros((m, n_s)) for _ in range(beam_width)]\n",
    "    c = [np.zeros((m, n_s)) for _ in range(beam_width)]\n",
    "    Y_prev = [np.zeros((m, 1, vocab_size)) for _ in range(beam_width)]\n",
    "    out = [None for _ in range(beam_width)]\n",
    "    candidates = [[] for _ in range(beam_width)]\n",
    "    \n",
    "    for t in range(Ty):\n",
    "        for i in range(beam_width):\n",
    "            out[i] = prob[i] * models[i].predict([a, s[i], c[i], Y_prev[i]])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"BACKGROUND\tThe emergence of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, inference_model, vocab_to_int, int_to_vocab):\n",
    "    data = np.array(transform_data(text, vocab_to_int))\n",
    "    data = to_categorical(data, num_classes=vocab_size)\n",
    "    prediction = np.array(inference_model.predict([data, np.zeros((data.shape[0], n_s)), np.zeros((data.shape[0], n_s)),\n",
    "                                                   np.zeros((data.shape[0], 1, vocab_size))], batch_size=100))\n",
    "    prediction = prediction.swapaxes(0, 1)\n",
    "    t = prediction[0, 13, :]\n",
    "    prediction = np.argmax(prediction, axis=-1)\n",
    "    prediction = [\"\".join(list(map(lambda x: int_to_vocab[x], i))) for i in prediction]\n",
    "    return prediction, t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, t = predict(text, inference_model, vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [(int_to_vocab[index], el) for index, el in enumerate(t)]\n",
    "sorted(t1, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "def z():\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
