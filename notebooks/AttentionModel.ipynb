{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..\\\\')\n",
    "import nltk\n",
    "import os\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Reshape\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from scripts.attention_utils import softmax\n",
    "from scripts.attention_preprocessing import load_data, transform_data\n",
    "from scripts.data_loader import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int, int_to_vocab = load_data(\"..//data//train.txt\", firstn=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"..\\\\data\\\\dicts\", 'wb') as file:\n",
    "#     pickle.dump((vocab_to_int, int_to_vocab), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"..\\\\data\\\\dicts\", 'rb') as file:\n",
    "    vocab_to_int, int_to_vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 200 # input sequence length\n",
    "Ty = 200 # output sequence length\n",
    "vocab_size = len(vocab_to_int) # number of unique characters\n",
    "n_a = 32 # number of neurons in single LSTM in encoder\n",
    "n_s = 64 # number of neurons in single LSTM in decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeat_layer = RepeatVector(Tx)\n",
    "concatenate_layer = Concatenate(axis=-1)\n",
    "dense_layer_1 = Dense(10, activation = \"tanh\")\n",
    "dense_layer_2 = Dense(1, activation = \"relu\")\n",
    "activation_layer = Activation(softmax, name='attention_weights')\n",
    "dot_prod_layer = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Args:\n",
    "    a (np.array): hidden state output of the Bi-LSTM, numpy-array of shape (?, Tx, 2*n_a)\n",
    "    s_prev (np.array): previous hidden state of the (post-attention) LSTM, numpy-array of shape (?, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context (np.array): context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeat_layer(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenate_layer([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e.\n",
    "    e = dense_layer_1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies.\n",
    "    energies = dense_layer_2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activation_layer(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dot_prod_layer([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model for training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_layer = Reshape((1, vocab_size))\n",
    "concatenate_layer_1 = Concatenate(axis=-1)\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(vocab_size, activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder part of the model\n",
    "X = Input(shape=(Tx, vocab_size), name='X')\n",
    "a = Bidirectional(LSTM(n_a, return_sequences=True))(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = Input(shape=(n_s,), name='s0')\n",
    "c0 = Input(shape=(n_s,), name='c0')\n",
    "Y_true = Input(shape=(Ty, vocab_size), name='Y_true')\n",
    "s = s0\n",
    "c = c0\n",
    "\n",
    "# Initialize empty list of outputs\n",
    "outputs = []\n",
    "\n",
    "for t in range(Ty):\n",
    "\n",
    "    # Perform one step of the attention mechanism to get back the context vector at step t\n",
    "    context = one_step_attention(a, s) # context.shape  = (?, 1, 2*n_a)\n",
    "    y_true = Lambda(lambda x: x[:, t, :])(Y_true) # y_true.shape = (?, vocab_size)\n",
    "    y_true = reshape_layer(y_true) # y_true.shape = (?, 1, vocab_size)\n",
    "    context = concatenate_layer_1([y_true, context])\n",
    "    # Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "    s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "\n",
    "    # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "    out = output_layer(s)\n",
    "\n",
    "    outputs.append(out)\n",
    "\n",
    "training_model = Model(inputs=[X, s0, c0, Y_true], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(0.0001, decay=0.001, clipnorm=10.0)\n",
    "training_model.compile(opt, 'categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filenames = [str(x) for x in range(22000)]\n",
    "generator = DataGenerator(filenames, filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.fit_generator(generator=generator, steps_per_epoch=500, epochs=1, workers=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.load_weights(\"..\\\\models\\\\attention_model\\\\attention.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.save_weights(\"..\\\\models\\\\attention_model\\\\attention.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=[X], outputs=[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_inference_model(vocab_size):\n",
    "    \"\"\"\n",
    "    Return decoder that makes one step inference\n",
    "    \n",
    "    Args:\n",
    "    vocab_size (int): number of distinct characters for output\n",
    "    Returns:\n",
    "    inference_model (Model): keras model that makes one step inference\n",
    "    \"\"\"\n",
    "    a = Input(shape=(Tx, 2*n_a), name='a')\n",
    "    Y_prev = Input(shape=(1, vocab_size), name='Y_inf')\n",
    "    y_prev = Y_prev\n",
    "    \n",
    "    context = one_step_attention(a, s0)\n",
    "    context = concatenate_layer_1([y_prev, context])\n",
    "    s, _, c = post_activation_LSTM_cell(context, initial_state=[s0, c0])\n",
    "    \n",
    "    out = output_layer(s)\n",
    "    \n",
    "    return Model(inputs=[a, s0, c0, Y_prev], outputs=[out, s, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 5\n",
    "inf_model = one_step_inference_model(vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(X, beam_width, vocab_size, Ty, n_s):\n",
    "    \"\"\"\n",
    "    Performs beam search among outputs of inference model\n",
    "    \n",
    "    Args:\n",
    "    X (numpy.ndarray): sentence in a form of numpy arrays X.shape = (m, Tx, vocab_size)\n",
    "    beam_width (int): number of best candidates to choose\n",
    "    vocab_size (int): number of distinct characters for output\n",
    "    Ty (int): number of time steps to perform\n",
    "    n_s (int): number of neurons in post attention LSTM\n",
    "    Returns:\n",
    "    characters (numpy.ndarray): output sentences\n",
    "    \n",
    "    \"\"\"\n",
    "    m = X.shape[0] # batch_size\n",
    "    \n",
    "    a = encoder.predict(x=[X])\n",
    "    \n",
    "    # perform first prediction\n",
    "    out, s, c = inf_model.predict([a, np.zeros((m, n_s)), np.zeros((m, n_s)), np.zeros((m, 1, vocab_size))])\n",
    "    a = np.concatenate([a for _ in range(beam_width)])\n",
    "    # define variables needed to save computations\n",
    "    prev_symbol_index = np.zeros((m, Ty, beam_width))\n",
    "    curr_symbol = np.zeros((m, Ty, beam_width))\n",
    "    s = np.concatenate([s for _ in range(beam_width)]) # s.shape = (m * beam_width, n_s)\n",
    "    c = np.concatenate([c for _ in range(beam_width)])\n",
    "    prob = np.log(np.partition(out, out.shape[-1] - beam_width, axis=-1)[:, -beam_width:]) # prob.shape = (m, beam_width)\n",
    "    curr_symbol[:, 0, :] = np.argpartition(out,  out.shape[-1] - beam_width, axis=-1)[:, -beam_width:]\n",
    "    # Y.shape = (m * beam_width, 1, vocab_size)\n",
    "    Y = np.expand_dims(np.concatenate([to_categorical(x, vocab_size) \n",
    "                                       for x in curr_symbol[:, 0, :].swapaxes(0, 1)], axis=0), axis=-2)\n",
    "    \n",
    "    for t in range(1, Ty):\n",
    "        # run predictions for all candidates\n",
    "        out_temp, s_temp, c_temp = inf_model.predict([a, s, c, Y]) #out_temp.shape = (m * beam_width, vocab_size)\n",
    "    \n",
    "        out_temp = np.reshape(out_temp[reshape_indices(m, beam_width), :], \n",
    "                              (m, beam_width, vocab_size)).swapaxes(1, 2).swapaxes(0, 1) #out_temp.shape = (vocab_size, m, beam_width)\n",
    "        \n",
    "        out_temp = np.reshape((np.log(out_temp) + prob).swapaxes(0, 1), (m, vocab_size * beam_width))\n",
    "        \n",
    "        \n",
    "        # choose top beam_width candidates\n",
    "        prob = np.partition(out_temp,  out_temp.shape[-1] - beam_width, axis=-1)[:, -beam_width:] # prob.shape = (m, beam_width)\n",
    "        indices = np.argpartition(out_temp,  out_temp.shape[-1] - beam_width, axis=-1)[:, -beam_width:]\n",
    "        prev_symbol_index[:, t, :] = indices % beam_width\n",
    "        curr_symbol[:, t, :] = indices // beam_width\n",
    "    \n",
    "        # prepare next inputs\n",
    "        input_indices = choose_activation(prev_symbol_index[:, t, :].swapaxes(0, 1).flatten().astype(int), m)\n",
    "        s = s_temp[input_indices, :]\n",
    "        c = c_temp[input_indices, :]\n",
    "        Y =  np.expand_dims(np.concatenate([to_categorical(x, vocab_size) \n",
    "                                       for x in curr_symbol[:, t, :].swapaxes(0, 1)], axis=0), axis=-2)\n",
    "    \n",
    "    # find output of the beam search\n",
    "    characters = np.zeros((m, Ty))\n",
    "    index = np.argmax(prob, axis=-1).astype(int)\n",
    "    characters[:, Ty - 1] = curr_symbol[:, Ty - 1, index].diagonal() # characters[:, Ty - 1].shape = (m, )\n",
    "    \n",
    "    for i in range(Ty - 2, -1, -1):\n",
    "        index = prev_symbol_index[:, i + 1, index].diagonal().astype(int)\n",
    "        characters[:, i] = curr_symbol[:, i, index].diagonal()\n",
    "        \n",
    "    return characters\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict(text, vocab_to_int, int_to_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 1, 4, 2, 5]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reshape_indices(a, b):\n",
    "    return [i + a*j  for i in range(a) for j in range(b)]\n",
    "reshape_indices(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_activation(indices, m):\n",
    "    return [m * indices[i] + i%m for i in range(len(indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This paper discribes the design and evaluation of Positive Outlook an online program aiming to enhance the selfmanagement skills of gay men living with .<EOS><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(list(map(lambda x: int_to_vocab[x], prediction[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The emergencee of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .\n",
    "BACKGROUND\tThis paper djscribes the design and evaluation of Positive Outlook , an online program aiming to enhance the self-management skills of gay men living with HIV .\n",
    "METHODS\tThis study is designed as a randomised controlled trial in which men living with HIV in Australia will be assigned to either an intervention group or usual care control group .\n",
    "METHODS\tThe intervention group will participate in the online group program ` Positive Outlook ' .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, vocab_to_int, int_to_vocab):\n",
    "    data = transform_data(text, vocab_to_int)\n",
    "    data = to_categorical(data, num_classes=vocab_size)\n",
    "    \n",
    "    prediction = beam_search(data, 10, vocab_size, Ty, n_s)\n",
    "#     for i in len(prediction):\n",
    "#         prediction[i] = prediction[i].swapaxes(0, 1)\n",
    "#         prediction[i] = np.argmax(prediction[i], axis=-1)\n",
    "#         prediction[i] = [\"\".join(list(map(lambda x: int_to_vocab[x], j))) for j in prediction[i]]\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
